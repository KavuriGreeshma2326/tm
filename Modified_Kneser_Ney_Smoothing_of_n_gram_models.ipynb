{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyORjP1p6cAqg/551vUi8mch",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KavuriGreeshma2326/tm/blob/main/Modified_Kneser_Ney_Smoothing_of_n_gram_models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u9Nox2raMDAP"
      },
      "outputs": [],
      "source": [
        "# Cell 0 - imports\n",
        "from collections import Counter, defaultdict\n",
        "import math\n",
        "import itertools\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 1 - toy corpus (replace with your own file in Colab)\n",
        "# If you have a large corpus, upload it to Colab (Files -> upload) or mount Google Drive.\n",
        "\n",
        "toy_corpus = [\n",
        "    \"the quick brown fox jumps over the lazy dog\",\n",
        "    \"the quick brown fox\",\n",
        "    \"the lazy dog sleeps\",\n",
        "    \"the dog barks at the fox\",\n",
        "    \"a quick dog outpaces the fox\",\n",
        "    \"dogs and foxes are different\",\n",
        "    \"the quick dog and the quick fox\"\n",
        "]\n",
        "\n",
        "# Example: how to load from uploaded text file (one sentence per line)\n",
        "# from google.colab import files\n",
        "# uploaded = files.upload()   # then read the uploaded filename into `lines`\n",
        "\n",
        "print(\"Toy corpus sentences:\", len(toy_corpus))\n",
        "for s in toy_corpus[:5]:\n",
        "    print(\"  \", s)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g0qkoGvhMiIS",
        "outputId": "8d5852cf-8046-47e6-b301-7be0f73a226b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Toy corpus sentences: 7\n",
            "   the quick brown fox jumps over the lazy dog\n",
            "   the quick brown fox\n",
            "   the lazy dog sleeps\n",
            "   the dog barks at the fox\n",
            "   a quick dog outpaces the fox\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 2 - tokenize, build vocabulary with <UNK> for rare words\n",
        "def simple_tokenize(s):\n",
        "    return s.lower().strip().split()\n",
        "\n",
        "def replace_rare_with_unk(sentences, min_freq=1):\n",
        "    # Build raw word frequencies\n",
        "    freq = Counter()\n",
        "    tokenized = []\n",
        "    for s in sentences:\n",
        "        toks = simple_tokenize(s)\n",
        "        tokenized.append(toks)\n",
        "        freq.update(toks)\n",
        "    vocab = set(w for w,c in freq.items() if c > min_freq)  # > min_freq keep more words\n",
        "    # Always include <UNK>, <s>, </s>\n",
        "    vocab.add(\"<UNK>\")\n",
        "    vocab.add(\"<s>\")\n",
        "    vocab.add(\"</s>\")\n",
        "    # replace rare words in tokenized sentences\n",
        "    replaced = []\n",
        "    for toks in tokenized:\n",
        "        replaced.append([w if w in vocab else \"<UNK>\" for w in toks])\n",
        "    return replaced, vocab, freq\n",
        "\n",
        "# use the toy corpus:\n",
        "tokenized, vocab, freq = replace_rare_with_unk(toy_corpus, min_freq=0)  # min_freq=0 -> keep all\n",
        "print(\"Vocab size:\", len(vocab))\n",
        "print(\"Example tokenized:\", tokenized[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jlZGLQQHMnOv",
        "outputId": "639a1062-3e51-4352-80a1-7e3f9c14985f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocab size: 21\n",
            "Example tokenized: ['the', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 3 - build ngram counts\n",
        "def build_ngram_counts(tokenized_sentences, N):\n",
        "    \"\"\"Return dict: counts[k] is Counter of k-grams (k=1..N) as tuples.\"\"\"\n",
        "    counts = {k: Counter() for k in range(1, N+1)}\n",
        "    contexts = defaultdict(set)  # for continuation counts if needed\n",
        "    for toks in tokenized_sentences:\n",
        "        # pad with (N-1) start symbols\n",
        "        padded = [\"<s>\"]*(N-1) + toks + [\"</s>\"]\n",
        "        L = len(padded)\n",
        "        for k in range(1, N+1):\n",
        "            for i in range(L - k + 1):\n",
        "                ngram = tuple(padded[i:i+k])\n",
        "                counts[k][ngram] += 1\n",
        "    return counts\n",
        "\n",
        "N = 3   # example: trigram model\n",
        "counts = build_ngram_counts(tokenized, N)\n",
        "print(\"Unigram types:\", len(counts[1]), \"Bigram types:\", len(counts[2]), \"Trigram types:\", len(counts[3]))\n",
        "# show a few counts\n",
        "print(\"Example bigrams:\", list(counts[2].items())[:10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PD8SgDAIMs9f",
        "outputId": "d4bb4aa7-8e77-4f4c-a997-6af506ecbeb8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unigram types: 20 Bigram types: 33 Trigram types: 38\n",
            "Example bigrams: [(('<s>', '<s>'), 7), (('<s>', 'the'), 5), (('the', 'quick'), 4), (('quick', 'brown'), 2), (('brown', 'fox'), 2), (('fox', 'jumps'), 1), (('jumps', 'over'), 1), (('over', 'the'), 1), (('the', 'lazy'), 2), (('lazy', 'dog'), 2)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 4 - compute D1,D2,D3+ for a given order using Chen & Goodman formulas\n",
        "def compute_D_discounts(ngram_counter):\n",
        "    # n1..n4 are the counts of ngram types that occur exactly 1,2,3,4 times\n",
        "    counts = list(ngram_counter.values())\n",
        "    n1 = sum(1 for c in counts if c == 1)\n",
        "    n2 = sum(1 for c in counts if c == 2)\n",
        "    n3 = sum(1 for c in counts if c == 3)\n",
        "    n4 = sum(1 for c in counts if c == 4)\n",
        "    # Y = n1 / (n1 + 2*n2)\n",
        "    denom = (n1 + 2*n2)\n",
        "    Y = (n1 / denom) if denom > 0 else 0.0\n",
        "    # fallback handling: if we don't have enough counts, use a typical small discount\n",
        "    if n1 == 0 or n2 == 0:\n",
        "        # safe fallback (common practice)\n",
        "        D1 = D2 = D3 = 0.75\n",
        "    else:\n",
        "        D1 = 1.0 - 2.0 * Y * (n2 / n1) if n1 > 0 else 0.0\n",
        "        D2 = 2.0 - 3.0 * Y * (n3 / n2) if n2 > 0 else D1\n",
        "        D3 = 3.0 - 4.0 * Y * (n4 / n3) if n3 > 0 else D2\n",
        "        # ensure non-negative and reasonable\n",
        "        D1 = max(0.0, min(D1, 2.0))\n",
        "        D2 = max(0.0, min(D2, 3.0))\n",
        "        D3 = max(0.0, min(D3, 4.0))\n",
        "    return (D1, D2, D3)\n",
        "\n",
        "discounts = {}\n",
        "for k in range(1, N+1):\n",
        "    discounts[k] = compute_D_discounts(counts[k])\n",
        "    print(f\"Order {k}: D1,D2,D3+ = {discounts[k]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IHz4uLcTMxnM",
        "outputId": "c6ff378f-c6a0-4850-9585-0b125a7c6bc7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Order 1: D1,D2,D3+ = (0.6470588235294118, 2.0, 2.0)\n",
            "Order 2: D1,D2,D3+ = (0.6571428571428571, 2.0, 2.0)\n",
            "Order 3: D1,D2,D3+ = (0.8, 1.4, 3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 5 - prepare context-level N1,N2,N3+ precomputation and a model class\n",
        "def precompute_context_stats(counts, N):\n",
        "    \"\"\"\n",
        "    For each context (length L = k-1) in order k,\n",
        "    compute N1,N2,N3plus for the continuations.\n",
        "    Returns: contexts_stats[k][context] = (N1,N2,N3plus, total_count)\n",
        "    where k ranges 2..N (because contexts relate to n-grams of length k)\n",
        "    \"\"\"\n",
        "    contexts_stats = {k: {} for k in range(2, N+1)}\n",
        "    for k in range(2, N+1):\n",
        "        ctr = counts[k]\n",
        "        # group by prefix (context)\n",
        "        by_context = defaultdict(list)\n",
        "        for ngram, c in ctr.items():\n",
        "            prefix = ngram[:-1]\n",
        "            by_context[prefix].append(c)\n",
        "        # compute stats\n",
        "        for prefix, cont_counts in by_context.items():\n",
        "            N1 = sum(1 for x in cont_counts if x == 1)\n",
        "            N2 = sum(1 for x in cont_counts if x == 2)\n",
        "            N3 = sum(1 for x in cont_counts if x >= 3)\n",
        "            total = sum(cont_counts)\n",
        "            contexts_stats[k][prefix] = (N1, N2, N3, total)\n",
        "    return contexts_stats\n",
        "\n",
        "contexts_stats = precompute_context_stats(counts, N)\n",
        "print(\"Example context stats (some bigram contexts):\")\n",
        "for k in [2,3]:\n",
        "    for i, (ctx, stats) in enumerate(contexts_stats[k].items()):\n",
        "        print(f\" order {k} context {ctx}: N1,N2,N3+,total = {stats}\")\n",
        "        if i>4: break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0fsBOParM208",
        "outputId": "19376659-d005-4682-a92c-17fc75034e3b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Example context stats (some bigram contexts):\n",
            " order 2 context ('<s>',): N1,N2,N3+,total = (2, 0, 2, 14)\n",
            " order 2 context ('the',): N1,N2,N3+,total = (1, 2, 1, 9)\n",
            " order 2 context ('quick',): N1,N2,N3+,total = (1, 2, 0, 5)\n",
            " order 2 context ('brown',): N1,N2,N3+,total = (0, 1, 0, 2)\n",
            " order 2 context ('fox',): N1,N2,N3+,total = (1, 0, 1, 5)\n",
            " order 2 context ('jumps',): N1,N2,N3+,total = (1, 0, 0, 1)\n",
            " order 3 context ('<s>', '<s>'): N1,N2,N3+,total = (2, 0, 1, 7)\n",
            " order 3 context ('<s>', 'the'): N1,N2,N3+,total = (2, 0, 1, 5)\n",
            " order 3 context ('the', 'quick'): N1,N2,N3+,total = (2, 1, 0, 4)\n",
            " order 3 context ('quick', 'brown'): N1,N2,N3+,total = (0, 1, 0, 2)\n",
            " order 3 context ('brown', 'fox'): N1,N2,N3+,total = (2, 0, 0, 2)\n",
            " order 3 context ('fox', 'jumps'): N1,N2,N3+,total = (1, 0, 0, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 6 - continuation counts used for the unigram base distribution\n",
        "def compute_unigram_continuation(counts):\n",
        "    # For each unigram token w, continuation_count[w] = # distinct left contexts that precede w in bigrams\n",
        "    bigrams = counts[2]\n",
        "    left_contexts = defaultdict(set)\n",
        "    for (w_prev, w), c in bigrams.items():\n",
        "        left_contexts[w].add(w_prev)\n",
        "    cont_count = {w: len(left_contexts[w]) for w in left_contexts}\n",
        "    total_types = len(bigrams)  # total distinct bigram types\n",
        "    # If some word never appears as second element, it will not be in cont_count -> 0\n",
        "    return cont_count, total_types\n",
        "\n",
        "cont_count, total_bigram_types = compute_unigram_continuation(counts)\n",
        "print(\"Unique bigram-types (for continuation):\", total_bigram_types)\n",
        "print(\"Example continuation counts:\", list(cont_count.items())[:10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XxLAhbVjNFxZ",
        "outputId": "3093ecaa-38e5-4647-b9c7-d8871fd5c349"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unique bigram-types (for continuation): 33\n",
            "Example continuation counts: [('<s>', 1), ('the', 5), ('quick', 2), ('brown', 1), ('fox', 3), ('jumps', 1), ('over', 1), ('lazy', 1), ('dog', 3), ('</s>', 4)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 7 - probability function using recursion (top-down)\n",
        "class ModKneserNey:\n",
        "    def __init__(self, counts, discounts, contexts_stats, cont_count, total_bigram_types, N):\n",
        "        self.counts = counts\n",
        "        self.discounts = discounts\n",
        "        self.contexts_stats = contexts_stats\n",
        "        self.cont_count = cont_count\n",
        "        self.total_bigram_types = total_bigram_types\n",
        "        self.N = N\n",
        "\n",
        "    def discount_for_count(self, order, c):\n",
        "        # use D1 for c==1, D2 for c==2, D3+ for c>=3\n",
        "        if c == 0:\n",
        "            return 0.0\n",
        "        D1, D2, D3 = self.discounts[order]\n",
        "        if c == 1:\n",
        "            return D1\n",
        "        elif c == 2:\n",
        "            return D2\n",
        "        else:\n",
        "            return D3\n",
        "\n",
        "    def p_continuation_unigram(self, w):\n",
        "        # base Kneser-Ney unigram = continuation_count(w) / total number of bigram types\n",
        "        return (self.cont_count.get(w, 0) / self.total_bigram_types) if self.total_bigram_types>0 else 0.0\n",
        "\n",
        "    def prob(self, w, context):\n",
        "        \"\"\"\n",
        "        w: token (string)\n",
        "        context: tuple of tokens length L where L <= N-1. If length L < N-1,\n",
        "                 we still apply same recursion on smaller contexts.\n",
        "        Returns: smoothed probability p(w | context)\n",
        "        \"\"\"\n",
        "        L = len(context)\n",
        "        if L == 0:\n",
        "            return self.p_continuation_unigram(w)\n",
        "        order = L + 1  # we are looking up ngrams of length L+1\n",
        "        c_context = 0\n",
        "        # total count of the context (sum of counts of full ngrams starting with context)\n",
        "        # contexts_stats[order] stores this as 4th element if the context exists\n",
        "        c_context = self.contexts_stats.get(order, {}).get(context, (0,0,0,0))[3]\n",
        "\n",
        "        # count of the exact ngram\n",
        "        ngram = tuple(context) + (w,)\n",
        "        c_ngram = self.counts[order].get(ngram, 0)\n",
        "\n",
        "        if c_context == 0:\n",
        "            # If context never seen, backoff to shorter context\n",
        "            return self.prob(w, context[1:])  # shorten left-most token\n",
        "        # discounted mass numerator\n",
        "        D_used = self.discount_for_count(order, c_ngram if c_ngram>0 else 0)\n",
        "        numerator = max(c_ngram - D_used, 0.0) / c_context\n",
        "\n",
        "        # compute gamma(context)\n",
        "        N1, N2, N3, _ = self.contexts_stats.get(order, {}).get(context, (0,0,0,0))\n",
        "        D1, D2, D3 = self.discounts[order]\n",
        "        gamma = (D1 * N1 + D2 * N2 + D3 * N3) / c_context\n",
        "\n",
        "        # recursively get lower-order probability\n",
        "        lower_prob = self.prob(w, context[1:])  # drop leftmost token\n",
        "        return numerator + gamma * lower_prob\n",
        "\n",
        "# instantiate model\n",
        "model = ModKneserNey(counts, discounts, contexts_stats, cont_count, total_bigram_types, N)\n",
        "\n",
        "# quick test: get probability of some tokens\n",
        "test_ctx = (\"the\",\"quick\")   # trigram context (N-1 length)\n",
        "print(\"P('fox' | 'the quick') =\", model.prob(\"fox\", test_ctx))\n",
        "print(\"P('dog' | 'quick dog') =\", model.prob(\"dog\", (\"quick\",\"dog\")) )\n",
        "print(\"P('the' | empty) unigram continuation:\", model.prob(\"the\", ()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WMYp4F68NONx",
        "outputId": "fe415635-4885-4936-d446-f1617d9dfe75"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "P('fox' | 'the quick') = 0.16493506493506493\n",
            "P('dog' | 'quick dog') = 0.047792207792207796\n",
            "P('the' | empty) unigram continuation: 0.15151515151515152\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 8 - sentence logprob and perplexity\n",
        "def sentence_logprob(model, tokens, N):\n",
        "    padded = [\"<s>\"]*(N-1) + tokens + [\"</s>\"]\n",
        "    L = len(padded)\n",
        "    total_logp = 0.0\n",
        "    total_words = 0\n",
        "    for i in range(N-1, L):\n",
        "        context = tuple(padded[i-(N-1):i])\n",
        "        w = padded[i]\n",
        "        p = model.prob(w, context)\n",
        "        if p <= 0:\n",
        "            # extremely small probability to avoid -inf; indicates OOV/unseen pattern\n",
        "            # in practice, ensure <UNK> is present in vocab and trained on\n",
        "            p = 1e-12\n",
        "        total_logp += math.log(p)\n",
        "        total_words += 1\n",
        "    return total_logp, total_words\n",
        "\n",
        "def perplexity(model, tokenized_sentences, N):\n",
        "    tot_logp = 0.0\n",
        "    tot_words = 0\n",
        "    for toks in tokenized_sentences:\n",
        "        logp, nw = sentence_logprob(model, toks, N)\n",
        "        tot_logp += logp\n",
        "        tot_words += nw\n",
        "    return math.exp(-tot_logp / tot_words) if tot_words>0 else float('inf')\n",
        "\n",
        "pp = perplexity(model, tokenized, N)\n",
        "print(\"Perplexity on toy corpus (training set):\", pp)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0y_yxdWBNUvk",
        "outputId": "dc64e27c-a5db-4e10-aaae-fb64aac3d601"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Perplexity on toy corpus (training set): 3.346631994544345\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 9 - next word distribution for a given context\n",
        "def top_k_predictions(model, context, k=10):\n",
        "    # consider vocabulary from unigram counts\n",
        "    vocab = set([w[0] if isinstance(w, tuple) else w for w in [ng[0:] for ng in model.counts[1].keys()]])\n",
        "    # but counts[1].keys() are tuples, so extract the unigram strings:\n",
        "    vocab = set([t[0] for t in model.counts[1].keys()])\n",
        "    scores = []\n",
        "    for w in vocab:\n",
        "        p = model.prob(w, context)\n",
        "        scores.append((p,w))\n",
        "    scores.sort(reverse=True)\n",
        "    return scores[:k]\n",
        "\n",
        "ctx = (\"the\",\"quick\")\n",
        "print(\"Top predictions for context\", ctx)\n",
        "for p,w in top_k_predictions(model, ctx, k=10):\n",
        "    print(f\"  {w:12}  p={p:.6f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "633Nhnb1NYX0",
        "outputId": "91d9e367-3c7e-4ab5-b743-6919ecdb232c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top predictions for context ('the', 'quick')\n",
            "  brown         p=0.171169\n",
            "  fox           p=0.164935\n",
            "  dog           p=0.113506\n",
            "  the           p=0.105844\n",
            "  </s>          p=0.084675\n",
            "  quick         p=0.042338\n",
            "  and           p=0.042338\n",
            "  sleeps        p=0.021169\n",
            "  over          p=0.021169\n",
            "  outpaces      p=0.021169\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 10 - 10-fold cross validation splits\n",
        "def k_fold_split(data, k=10, seed=42):\n",
        "    random.Random(seed).shuffle(data)\n",
        "    fold_size = len(data) // k\n",
        "    folds = []\n",
        "    for i in range(k):\n",
        "        test = data[i*fold_size : (i+1)*fold_size]\n",
        "        train = data[:i*fold_size] + data[(i+1)*fold_size:]\n",
        "        folds.append((train, test))\n",
        "    return folds"
      ],
      "metadata": {
        "id": "RrlsIopKO6YF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 11 - different discount selection strategies\n",
        "\n",
        "def compute_discounts_count(ngram_counter):\n",
        "    # base modkn-count (already implemented)\n",
        "    return compute_D_discounts(ngram_counter)\n",
        "\n",
        "def compute_discounts_extend(ngram_counter, contexts_stats):\n",
        "    # like count, but discounts depend on extended contexts\n",
        "    # for simplicity, use same D formula, but based on continuation counts\n",
        "    counts = []\n",
        "    for ctx, (N1,N2,N3,Ntotal) in contexts_stats.items():\n",
        "        counts.extend([1]*N1 + [2]*N2 + [3]*N3)\n",
        "    fake_counter = Counter(counts)\n",
        "    return compute_D_discounts(fake_counter)\n",
        "\n",
        "def compute_discounts_diffd(ngram_counter, contexts_stats):\n",
        "    # like extend, but based on number of extended contexts with counts 1,2,3,4\n",
        "    counts = []\n",
        "    for ctx, (N1,N2,N3,Ntotal) in contexts_stats.items():\n",
        "        counts.extend([1]*N1 + [2]*N2 + [3]*N3)\n",
        "    fake_counter = Counter(counts)\n",
        "    return compute_D_discounts(fake_counter)  # placeholder (diffd same as extend here)"
      ],
      "metadata": {
        "id": "5U6Lf7InO-Hz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# helper: tokenize test data using train vocab\n",
        "def tokenize_with_vocab(sentences, vocab):\n",
        "    processed = []\n",
        "    for s in sentences:\n",
        "        toks = simple_tokenize(s)\n",
        "        toks = [w if w in vocab else \"<UNK>\" for w in toks]\n",
        "        processed.append(toks)\n",
        "    return processed"
      ],
      "metadata": {
        "id": "kb-RT_T5PaPj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 12 - train and evaluate one fold\n",
        "def train_and_eval(train_corpus, test_corpus, N, variant=\"count\"):\n",
        "    # tokenize with UNK handling from training set\n",
        "    tokenized_train, vocab, freq = replace_rare_with_unk(train_corpus, min_freq=1)\n",
        "    counts = build_ngram_counts(tokenized_train, N)\n",
        "    contexts_stats = precompute_context_stats(counts, N)\n",
        "    cont_count, total_bigram_types = compute_unigram_continuation(counts)\n",
        "\n",
        "    # choose discounts\n",
        "    discounts = {}\n",
        "    for k in range(1, N+1):\n",
        "        if variant == \"count\":\n",
        "            discounts[k] = compute_discounts_count(counts[k])\n",
        "        elif variant in (\"extend\",\"diffd\"):\n",
        "            discounts[k] = compute_discounts_extend(counts[k], contexts_stats.get(k,{}))\n",
        "        else:\n",
        "            discounts[k] = compute_discounts_count(counts[k])  # fallback\n",
        "\n",
        "    model = ModKneserNey(counts, discounts, contexts_stats, cont_count, total_bigram_types, N)\n",
        "\n",
        "    # tokenize test using train vocab\n",
        "    test_tokenized = tokenize_with_vocab(test_corpus, vocab)\n",
        "    return perplexity(model, test_tokenized, N)"
      ],
      "metadata": {
        "id": "_210aiKrPBn-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 13 - run experiments\n",
        "def run_experiments(corpus, N_values=[3,4,5], k=10):\n",
        "    folds = k_fold_split(corpus, k=k)\n",
        "    results = {variant: {N: [] for N in N_values} for variant in [\"count\",\"extend\",\"diffd\"]}\n",
        "\n",
        "    for train, test in folds:\n",
        "        for variant in results.keys():\n",
        "            for N in N_values:\n",
        "                pp = train_and_eval(train, test, N, variant=variant)\n",
        "                results[variant][N].append(pp)\n",
        "    return results\n",
        "\n",
        "# Example run on toy corpus (small, just demo)\n",
        "exp_results = run_experiments(toy_corpus, N_values=[3], k=3)\n",
        "print(exp_results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X4dIAiDTPEs_",
        "outputId": "6ff7d619-b4f0-446d-ed02-428bb74805d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'count': {3: [5.712232000720976, 8.724294061653397, 4.685855580346316]}, 'extend': {3: [5.674508613012864, 7.252088517311338, 5.7408763207223705]}, 'diffd': {3: [5.674508613012864, 7.252088517311338, 5.7408763207223705]}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 14 - pretty table output\n",
        "import numpy as np\n",
        "from tabulate import tabulate\n",
        "\n",
        "def summarize_results(results):\n",
        "    rows = []\n",
        "    for variant, N_dict in results.items():\n",
        "        row = [variant]\n",
        "        for N, vals in N_dict.items():\n",
        "            arr = np.array(vals)\n",
        "            mean, std = arr.mean(), arr.std()\n",
        "            row.append(f\"{mean:.3f} ({std:.3f})\")\n",
        "        rows.append(row)\n",
        "    headers = [\"Model\"] + [f\"N={N}\" for N in list(next(iter(results.values())).keys())]\n",
        "    print(tabulate(rows, headers=headers, tablefmt=\"grid\"))\n",
        "\n",
        "# Example summary\n",
        "summarize_results(exp_results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bMnajonHPeg1",
        "outputId": "00a738b2-64d8-41de-ab2c-ff9624d292f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+---------------+\n",
            "| Model   | N=3           |\n",
            "+=========+===============+\n",
            "| count   | 6.374 (1.714) |\n",
            "+---------+---------------+\n",
            "| extend  | 6.222 (0.729) |\n",
            "+---------+---------------+\n",
            "| diffd   | 6.222 (0.729) |\n",
            "+---------+---------------+\n"
          ]
        }
      ]
    }
  ]
}